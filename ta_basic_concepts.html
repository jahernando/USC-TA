
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Revisiting the basic concepts &#8212; Statistics for Particle Physics</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Hypothesis testing, simple case" href="ta_hypothesis_test.html" />
    <link rel="prev" title="Neutrinos - USC" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Statistics for Particle Physics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Neutrinos - USC
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Revisiting the basic concepts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ta_hypothesis_test.html">
   Hypothesis testing, simple case
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ta_confidence_intervals.html">
   Confidence Intervals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ta_hypothesis_test_composite.html">
   Hypothesis testing, composite case
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/jahernando/USC-TA/master?urlpath=tree/docs/ta_basic_concepts.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/jahernando/USC-TA/blob/master/docs/ta_basic_concepts.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/jahernando/USC-TA"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jahernando/USC-TA/issues/new?title=Issue%20on%20page%20%2Fta_basic_concepts.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/ta_basic_concepts.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayes-theorem">
   1. Bayes’ Theorem
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kolmogorov-axioms">
     Kolmogorov axioms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Bayes’ theorem
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-density-functions">
   2. Probability density functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#common-probability-density-functions">
     Common probability density functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectation-values">
     Expectation values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood">
     Likelihood
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#frequentist-vs-bayesians">
   3. Frequentist vs Bayesians
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood-and-posterior-probability-an-example">
     Likelihood and posterior  probability - an example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#posterior-probability-in-bayesian-statistics">
     Posterior probability in bayesian statistics
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Revisiting the basic concepts</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayes-theorem">
   1. Bayes’ Theorem
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kolmogorov-axioms">
     Kolmogorov axioms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Bayes’ theorem
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-density-functions">
   2. Probability density functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#common-probability-density-functions">
     Common probability density functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectation-values">
     Expectation values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood">
     Likelihood
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#frequentist-vs-bayesians">
   3. Frequentist vs Bayesians
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood-and-posterior-probability-an-example">
     Likelihood and posterior  probability - an example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#posterior-probability-in-bayesian-statistics">
     Posterior probability in bayesian statistics
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="revisiting-the-basic-concepts">
<h1>Revisiting the basic concepts<a class="headerlink" href="#revisiting-the-basic-concepts" title="Permalink to this heading">#</a></h1>
<p><em>Author: Jose A. Hernando</em>, January 2020</p>
<p><em>Edited by: Carlos Vázquez Sierra</em>, February 2023</p>
<p><em>Instituto Galego de Altas Enerxías. Universidade de Santiago de Compostela, Spain.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="nb">print</span><span class="p">(</span> <span class="s1">&#39; Last Execution &#39;</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">asctime</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Last Execution  Tue Feb  1 18:06:03 2022
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># general imports</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">reload_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2

<span class="c1"># numpy and matplotlib</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span>       <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">scipy.optimize</span>     <span class="k">as</span> <span class="nn">optimize</span> 

<span class="kn">import</span> <span class="nn">htintro_examples</span>  <span class="k">as</span> <span class="nn">htexam</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="s1">&#39;seaborn-colorblind&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="bayes-theorem">
<h2>1. Bayes’ Theorem<a class="headerlink" href="#bayes-theorem" title="Permalink to this heading">#</a></h2>
<div class="section" id="kolmogorov-axioms">
<h3>Kolmogorov axioms<a class="headerlink" href="#kolmogorov-axioms" title="Permalink to this heading">#</a></h3>
<p><strong>Nature is probabilistic.</strong></p>
<p>The starting point of probability are the <strong>Kolmogorov axioms</strong>:</p>
<ul class="simple">
<li><p>The probability for an event, <span class="math notranslate nohighlight">\(E\)</span>, is non negative <span class="math notranslate nohighlight">\(P(E)\ge 0\)</span>.</p></li>
<li><p>The probability for the entire space of possibilities, <span class="math notranslate nohighlight">\(\Omega\)</span>, is one, <span class="math notranslate nohighlight">\(P(\Omega) = 1\)</span>.</p></li>
<li><p>The probability for disjoint events, <span class="math notranslate nohighlight">\(E_1,\dots,E_n\)</span>, is additive, <span class="math notranslate nohighlight">\(P(E_1  \dots  \cup E_n) = \sum_{i=1,n} P(E_i)\)</span>.</p></li>
</ul>
<p>From there we obtain the following corollaries:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(P(A \cup B) = P(A) + P(B) - P(A \cap B) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(!E) = 1- P(E)\)</span></p>
<p>where <span class="math notranslate nohighlight">\(!E\)</span> is probability that <span class="math notranslate nohighlight">\(E\)</span> does not happen.</p>
</li>
</ul>
<p>A visual example of the Kolmogorov’s axioms and its colloraries:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p><a class="reference internal" href="_images/L1_kolmogorov_example.png"><img alt="_images/L1_kolmogorov_example.png" src="_images/L1_kolmogorov_example.png" style="width: 600px;" /></a></p></td>
</tr>
</tbody>
</table>
<p>which satisfies the following relations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A \cup B) = P(A) + P(B) = P(A) + P(!A) = P(\Omega) = 1\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(P(C \cup D) = P(C) + P(D) - P(C \cap D) = P (C) + P(D) - P(E)\)</span>.</p></li>
</ul>
</div>
<div class="section" id="id1">
<h3>Bayes’ theorem<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>The probability <span class="math notranslate nohighlight">\(P(A)\)</span> that an event happens is called <strong>marginal probability</strong>.</p>
<p>For example, the probability to roll a dice and get 3.</p>
<p>The probability that an event <span class="math notranslate nohighlight">\(A\)</span> happens if another one <span class="math notranslate nohighlight">\(B\)</span> has happened is called <strong>conditional probability</strong>, <span class="math notranslate nohighlight">\(P(A|B)\)</span>.</p>
<p>For example, the probability that rolling your dice is 3 if you know that the number obtained was odd, in that case <span class="math notranslate nohighlight">\(P(3|\mathrm{odd})=1/3\)</span>.</p>
<p>The conditional probabilities <span class="math notranslate nohighlight">\(P(A|B)\)</span> and <span class="math notranslate nohighlight">\(P(B|A)\)</span> are defined from the probabilities of both events A and B to happen and their marginal probabilities, this is:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A \cup B) = P(A|B) \, P(B)\)</span> , with <span class="math notranslate nohighlight">\(P(B) \neq 0\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B \cup A) = P(B|A) \, P(A)\)</span> , with <span class="math notranslate nohighlight">\(P(A) \neq 0\)</span>,</p></li>
</ul>
<p>The intersection probabilities between A and B are commutative, <span class="math notranslate nohighlight">\(P(A \cup B) = P(B \cup A)\)</span>, hence, both expressions are equivalent:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A|B) \, P(B) = P(B|A) \, P(A)\)</span>.</p></li>
</ul>
<p>The <strong>Bayes’ theorem</strong> relates both conditional probabilities: if <span class="math notranslate nohighlight">\(A,B\)</span> are two events with marginal probabilities <span class="math notranslate nohighlight">\(P(A),\, P(B)\)</span>, the conditional probabilities <span class="math notranslate nohighlight">\(P(A|B), \, P(B|A)\)</span> are:</p>
<div class="math notranslate nohighlight">
\[
P(A|B) \, P(B) = P(B|A)\,P(A)
\]</div>
<p><strong>Example:</strong> consider the experiment of rolling two dices, <span class="math notranslate nohighlight">\(A\)</span> is the case when the sum of both dices is 6, and <span class="math notranslate nohighlight">\(B\)</span> when one of them is 4. Verify the Bayes’ theorem.</p>
<p>The probability to get six adding both dices is <span class="math notranslate nohighlight">\(P(A)= 5/36\)</span>, possible outcomes are <span class="math notranslate nohighlight">\(\{(1,5),(2,4),(3,3),(4,2),(5,1)\}\)</span>.</p>
<p>The probability to get four when the addition is six is <span class="math notranslate nohighlight">\(P(B|A)=2/5\)</span>.</p>
<p>The probability to get a dice with four is <span class="math notranslate nohighlight">\(P(B)=11/36\)</span>.</p>
<p>The probability that both add six if one is four is <span class="math notranslate nohighlight">\(2/11\)</span>, therefore:</p>
<div class="math notranslate nohighlight">
\[
\frac{11}{36} \, \frac{2}{11} = \frac{2}{5} \, \frac{5}{36} = \frac{1}{18}
\]</div>
<p><strong>Exercise:</strong> In a low prevalence population, the probability of an individual to have HIV+ is 1 in 1000. Consider a medical test of HIV. If the patient is HIV+, the test is positive in 99.8 % of the times (true-positive), but if the person is HIV-, the test can wrongly be positive in 0.2 % of the times. After a test, a patient is diagnosed with HIV+, what is the probability that he has HIV +? Should the doctor repeat the test?</p>
<div class="math notranslate nohighlight">
\[
p(H+ | +) = \frac{p(+ | H+) p(H+)}{p(+)} =  \frac{p(+ | H+) p(H+)}{p(+ | H+) p (H+) + p(+ | H-) p (H-)}
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[
p(H+) = 10^{-3}, \; p(H-) = 1-10^{-3}, \; p(+ | H+) = 0.998, \; p(+ | H-) = 0.002 
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bayes_pax</span><span class="p">(</span><span class="n">pa</span><span class="p">,</span> <span class="n">pxa</span><span class="p">,</span> <span class="n">pxnoa</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">pxa</span> <span class="o">*</span> <span class="n">pa</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">pxa</span> <span class="o">*</span> <span class="n">pa</span> <span class="o">+</span> <span class="n">pxnoa</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pa</span><span class="p">))</span>

<span class="n">pax</span> <span class="o">=</span> <span class="n">bayes_pax</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">0.998</span><span class="p">,</span> <span class="mf">0.002</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.3331108144192256
</pre></div>
</div>
</div>
</div>
<p><strong>Exercise:</strong> A neutrino experiment has a problem with the DAQ, and 5% of the running time it does not observe interactions. The neutrinos are produced by a reactor nearby that operates 75% of the time. In this moment, the experiment does not observe neutrinos, what is the probability that the reactor is off?</p>
</div>
</div>
<div class="section" id="probability-density-functions">
<h2>2. Probability density functions<a class="headerlink" href="#probability-density-functions" title="Permalink to this heading">#</a></h2>
<p>Measurements are of probabilistic nature, there are <strong>random variables</strong> (rv).</p>
<p>The distribution probability, <span class="math notranslate nohighlight">\(g(x)\)</span>, that follow a rv is called <strong>probability density function</strong>, pdf.</p>
<p>If <span class="math notranslate nohighlight">\(x\)</span> takes discrete values, it is called <strong>probability mass function</strong>, pmf.</p>
<p>Here we will abuse the lenguage and call them both as ‘pdf’.</p>
<p>In most of the cases the pdfs depends on some parameters <span class="math notranslate nohighlight">\(\mu\)</span>, indicated as <span class="math notranslate nohighlight">\(g(x | \mu)\)</span>.</p>
<p>From the probability axioms we have:</p>
<div class="math notranslate nohighlight">
\[
\int_{-\infty}^{\infty} g(x) \, \mathrm{d}x = 1
\]</div>
<p>We define the cumulative density function, cdf, <span class="math notranslate nohighlight">\(F(x)\)</span>, as:</p>
<div class="math notranslate nohighlight">
\[
F(x) = \int_{-\infty}^{x} g(x) \, \mathrm{d}x
\]</div>
<div class="section" id="common-probability-density-functions">
<h3>Common probability density functions<a class="headerlink" href="#common-probability-density-functions" title="Permalink to this heading">#</a></h3>
<p>The most common distributions in HEP are:</p>
<ul class="simple">
<li><p><strong>Binomial</strong>. A event can happen with probability <span class="math notranslate nohighlight">\(p\)</span>. The probability to get <span class="math notranslate nohighlight">\(n\)</span> events if we try <span class="math notranslate nohighlight">\(N\)</span> times is given by the binomial pdf.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(n|N,p) = \frac{N!}{n! (N-n)!} p^n (1-p)^{N-n}
\]</div>
<ul class="simple">
<li><p><strong>Poisson</strong>. Number of events if we expect <span class="math notranslate nohighlight">\(\nu\)</span>, <em>i.e.</em> the number of interactions in a crossing of the LHC beams is modeled with a poisson. Nuclear decays are poisson distributed too.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
g(n|\nu) = \frac{\nu^n}{n!} e^{-\nu}
\]</div>
<ul class="simple">
<li><p><strong>Uniform</strong>. Equal probability to get <span class="math notranslate nohighlight">\(x\)</span> in an interval <span class="math notranslate nohighlight">\([a, b]\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x|a,b) = \frac{1}{(b-a)}
\]</div>
<ul class="simple">
<li><p><strong>Exponential</strong>. An event can happen in <span class="math notranslate nohighlight">\(x\)</span> with probability <span class="math notranslate nohighlight">\(1/\tau\)</span>. <em>i.e.</em> the decay time of particles.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x|\tau) = \frac{1}{\tau} e^{-x/\tau}
\]</div>
<ul class="simple">
<li><p><strong>Gaussian</strong> (or normal). Associated with the distributions of measurements.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
g(x|\mu,\sigma) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]</div>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(\chi^2_n\)</span></strong> with <span class="math notranslate nohighlight">\(n\)</span> degrees of freedom. Associated with goodness of fit.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x|n) = \frac{1}{2^{n/2} \Gamma(n/2)} x^{n/2-1}e^{-x/2}
\]</div>
<ul class="simple">
<li><p><strong>Breit-Wigner</strong>. Describes the distribution of masses in resonances.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x|\Gamma,x_0) = \frac{1}{\pi} \frac{\Gamma/2}{\Gamma^2/4 + (x-x_0)^2} 
\]</div>
<ul class="simple">
<li><p><strong>Beta</strong>. Associated with the measurement of probabilities. Used in Bayesian statistics.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x | \alpha,\beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1} (1-x)^{\beta-1}
\]</div>
<p>where <span class="math notranslate nohighlight">\(x, \alpha, \beta\)</span> are positive.</p>
<ul class="simple">
<li><p><strong>Gamma</strong>. Associated with Bayesian statistics and Poisson distributions.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x | \alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-x \beta}
\]</div>
<p>where <span class="math notranslate nohighlight">\(x, \alpha, \beta\)</span> are positive.</p>
<p>Some of the pdfs are nicely related:</p>
<p>In the case that an event is rare, has a small probability, <span class="math notranslate nohighlight">\(p\)</span>, but we do a large number of trials, <span class="math notranslate nohighlight">\(N\)</span>, the events distribution follows a binomial with <span class="math notranslate nohighlight">\(g(n | N, p)\)</span>, if <span class="math notranslate nohighlight">\( N \, p = \nu\)</span> is constant, and <span class="math notranslate nohighlight">\(N\)</span> is large enough and <span class="math notranslate nohighlight">\(p\)</span> small enough, the binomial is equivalent to a Poisson with mean <span class="math notranslate nohighlight">\(\nu\)</span>, <span class="math notranslate nohighlight">\(g(n | \nu)\)</span>.</p>
<p>For example nucleus decays follow this rule: large trials (<span class="math notranslate nohighlight">\(N\)</span>, number of nucleus) and small decay probability (<span class="math notranslate nohighlight">\(p\)</span>).</p>
<p>When <span class="math notranslate nohighlight">\(\nu\)</span> is “large”, 12 is enough, the Poisson transforms into Gaussian with <span class="math notranslate nohighlight">\(\mu=\nu, \sigma = \sqrt{\mu}\)</span>.</p>
<p>When we take n values of <span class="math notranslate nohighlight">\(x\)</span>, gaussian distributed, with mean <span class="math notranslate nohighlight">\(\mu\)</span> and sigma <span class="math notranslate nohighlight">\(\sigma\)</span>, and compute <span class="math notranslate nohighlight">\(\chi^2 = \sum_{i=1,n} \frac{(x_i -\mu)^2}{\sigma^2}\)</span>, it follows a chi-squared distribution with <span class="math notranslate nohighlight">\(n\)</span> degrees of freedom.</p>
<p><strong>Exercise:</strong> Compare a poisson distribution, with <span class="math notranslate nohighlight">\(\nu=p \, N\)</span> with a binomial when <span class="math notranslate nohighlight">\(N\)</span> is large and <span class="math notranslate nohighlight">\(p\)</span> is small.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">0.015</span><span class="p">;</span> <span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;binomial&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="n">p</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span> <span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;poisson&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;g(x)&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ta_basic_concepts_20_0.png" src="_images/ta_basic_concepts_20_0.png" />
</div>
</div>
<p><strong>Exercise:</strong> Compare a gaussian distribution, with <span class="math notranslate nohighlight">\(\mu=p \, N, \, \sigma = \sqrt{\mu}\)</span> with a binomial when <span class="math notranslate nohighlight">\(N\)</span> is large and <span class="math notranslate nohighlight">\(p\)</span> is small and <span class="math notranslate nohighlight">\(\mu = N p &gt; 50\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">,</span> <span class="mf">50.</span><span class="o">/</span><span class="mf">1e4</span><span class="p">;</span> <span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>            <span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span> <span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;binomial&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">N</span><span class="o">*</span><span class="n">p</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;g(x)&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ta_basic_concepts_22_0.png" src="_images/ta_basic_concepts_22_0.png" />
</div>
</div>
<p><strong>Exercise:</strong> Check that from a “large” <span class="math notranslate nohighlight">\(\nu\)</span> the poisson distribution is equivalent to a gaussian distribution.</p>
<p><strong>Exercise:</strong> Generate <span class="math notranslate nohighlight">\(m\)</span> datasets, each one with <span class="math notranslate nohighlight">\(n\)</span> data, <span class="math notranslate nohighlight">\(x_i\)</span> distributed random in <span class="math notranslate nohighlight">\([0, 1]\)</span> interval, sum the <span class="math notranslate nohighlight">\(n\)</span> numbers, <span class="math notranslate nohighlight">\(\sum_i^n x_i\)</span> and obtain the distribution of the <span class="math notranslate nohighlight">\(m\)</span> samples.</p>
<p><strong>Exercise:</strong> Generate <span class="math notranslate nohighlight">\(m\)</span> datasets, each with <span class="math notranslate nohighlight">\(n\)</span> data, <span class="math notranslate nohighlight">\(x_i\)</span>, normal distributed, compute its distance squared <span class="math notranslate nohighlight">\(\chi^2 = \sum_{i=1}^n x^2_i\)</span>, what is the distribution of <span class="math notranslate nohighlight">\(\chi^2\)</span>?</p>
<p><strong>Exercise:</strong> Generate <span class="math notranslate nohighlight">\(n\)</span> <span class="math notranslate nohighlight">\(x_i\)</span>-values, each one gaussian distributed with mean <span class="math notranslate nohighlight">\(\mu_i\)</span> and sigma <span class="math notranslate nohighlight">\(\sigma_i\)</span>, show that the sum <span class="math notranslate nohighlight">\(\sum_i x_i\)</span> is gaussian distributed with mean, <span class="math notranslate nohighlight">\(\mu = \sum_i \mu_i\)</span>, and sigma <span class="math notranslate nohighlight">\(\sigma^2 = \sum_i \sigma^2_i\)</span>.</p>
</div>
<div class="section" id="expectation-values">
<h3>Expectation values<a class="headerlink" href="#expectation-values" title="Permalink to this heading">#</a></h3>
<p>Given a <span class="math notranslate nohighlight">\(x\)</span> rv that follows a pdf, <span class="math notranslate nohighlight">\(\, g(x)\)</span>, and a function <span class="math notranslate nohighlight">\(f(x)\)</span> on <span class="math notranslate nohighlight">\(x\)</span>, we define the expected value of <span class="math notranslate nohighlight">\(g(x)\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
E[g(x)] \equiv \int f(x) \, g(x) \, \mathrm{d}x
\]</div>
<p>The <strong>mean</strong>, or average value, is the expected value of <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[
E[x] = \mu \equiv  \int x \, g(x) \, \mathrm{d}x
\]</div>
<p>The <strong>variance</strong> is the expected value of <span class="math notranslate nohighlight">\((x-\mu)^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
V[x] = \sigma^2 = E[(x-\mu)^2]= E[x^2]-\mu^2 \equiv \int (x-\mu)^2 \, g(x) \, \mathrm{d}x
\]</div>
<p>We call <strong>standard deviation</strong> to: <span class="math notranslate nohighlight">\(\sigma = \sqrt{V[x]}\)</span></p>
<p>The <strong>expected value</strong> is the <span class="math notranslate nohighlight">\(x\)</span> value with the highest probability <span class="math notranslate nohighlight">\(g(x)\)</span>.</p>
<p>The <strong>median</strong>, is the <span class="math notranslate nohighlight">\(x_{med}\)</span> value that divides the distribution in half,</p>
<div class="math notranslate nohighlight">
\[
\int_0^{x_{med}} g(x) \mathrm{d}x = 0.5
\]</div>
<p>For a symmetric pdf, the mean and median are the same.</p>
<p>If <span class="math notranslate nohighlight">\({\bf x}\)</span> is a vector, the pdf is a n-dimensional function.
We define the covariance element between <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> elements as:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{cov}[x_i,x_j] = \int x_i \, x_j \, g({\bf x}) \; \Pi_{i=1,n} \mathrm{d}x_i
\]</div>
<p>If the variables <span class="math notranslate nohighlight">\(x_i, \, x_j\)</span> variables are independent then the covariance is zero.
The contrary is not necessarily true.</p>
<p>We call <strong>marginal</strong> pdf when one or more rvs are integrated out, for example if we integrate <span class="math notranslate nohighlight">\(x_n\)</span>:
$<span class="math notranslate nohighlight">\(
g(x_1,\dots,x_{n-1}) = \int g({\bf x}) \mathrm{d}x_n
\)</span>$</p>
<p>We can “project” the pdf in one axis, that is get the marginal pdf for each variable <span class="math notranslate nohighlight">\(x_i\)</span> individually:
$<span class="math notranslate nohighlight">\(
g(x_j) = \int g({\bf x}) \, \Pi_{i=1,n; i \neq j} \, \mathrm{d}x_i
\)</span>$</p>
<p><strong>Exercise:</strong> Get the poisson distribution from a Gamma distribution.</p>
<p><strong>Exercise:</strong> Get an exponential distribution from a Gamma distribution.</p>
<p><strong>Exercise:</strong> Get the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution from a Gamma distribution.</p>
<p><strong>Exercise:</strong> Study the Beta distribution for different parameters of <span class="math notranslate nohighlight">\(\alpha, \beta\)</span> (start <span class="math notranslate nohighlight">\(\alpha=\beta=1\)</span>).</p>
</div>
<div class="section" id="likelihood">
<h3>Likelihood<a class="headerlink" href="#likelihood" title="Permalink to this heading">#</a></h3>
<p>A frequentist evaluate the likelihood of its data.</p>
<p>The <strong>probability density function</strong>, <em>pdf</em>, <span class="math notranslate nohighlight">\(g(x | \mu)\)</span>is the probability to measure <span class="math notranslate nohighlight">\(x\)</span>, which depends on the parameters <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>A <strong>likelihood</strong> is the probability evaluted on a observation data, <span class="math notranslate nohighlight">\(x\)</span>, (on data!)</p>
<p>For n, <span class="math notranslate nohighlight">\(x\)</span>, independent measurements that follow a pdf, <span class="math notranslate nohighlight">\(g(x | \mu)\)</span>, the likelihood is the product of the likelihood of each measurement <span class="math notranslate nohighlight">\(x_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(x | \mu) = \Pi_{i=1}^n g(x_i | \mu)
\]</div>
<p>The likelihood can be a very small number, for that reason, it is common to take the (Napierian!) logarithm, and called <strong>log likelihood</strong>:</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L}(x |\mu) = \sum_{i=1}^n \log \left(g(x_i |\mu) \right)
\]</div>
<p>If <span class="math notranslate nohighlight">\(\mu\)</span> is known, the likelihood is an real number, if not, <span class="math notranslate nohighlight">\(\mathcal{L}(x | \mu)\)</span> is a function of the <span class="math notranslate nohighlight">\(\mu\)</span> parameter(s).</p>
<p>Given data <span class="math notranslate nohighlight">\(x\)</span>, with <span class="math notranslate nohighlight">\(\mu\)</span> unknown, the likelihood is a function that depends on the parameter(s) <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}(x | \mu)\)</span>.</p>
<p>Frequentists usually estimate the best-parameters, <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>, as the parameters that maximize the likelihood (<span class="math notranslate nohighlight">\(\mathcal{L}(x | \hat{\mu})\)</span> is maximum).</p>
<p>But for convenience, we use instead:</p>
<div class="math notranslate nohighlight">
\[
-2 \log \mathcal{L}(x |\mu)
\]</div>
<p>And then <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> minimize the log-likelihood.</p>
</div>
</div>
<div class="section" id="frequentist-vs-bayesians">
<h2>3. Frequentist vs Bayesians<a class="headerlink" href="#frequentist-vs-bayesians" title="Permalink to this heading">#</a></h2>
<p>The is a great divide in statistics: frequentists vs Bayesians.</p>
<p><strong>Frequentists are inductive</strong>. <em>They compute the probability of an observation by repeating the same experiment many times</em>. They test is the data is compatible with the theory.</p>
<p><strong>Bayesians are deductive</strong>. <em>They compute the probability using the Bayes’ theorem.
They measure the credibility of a theory based on the data, but they need to assign first a prior probability based in a ‘reasonable’ initial guess</em>.</p>
<p>Frequentists measure a <strong>likelihood function</strong>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L} (x | \mu)
\]</div>
<p>Frequentists usually estimate the best-parameters, <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>, as the parameters that maximize the likelihood (<span class="math notranslate nohighlight">\(\mathcal{L}(x | \hat{\mu})\)</span> is maximum).</p>
<p>Bayesians measure a <strong>posterior probability</strong>:</p>
<div class="math notranslate nohighlight">
\[
p(\mu | x)
\]</div>
<p>using an initial prior <span class="math notranslate nohighlight">\(\pi(\mu)\)</span> probability and the Bayes’ theorem.</p>
<p>A conversation between a bayesian and a frequentist:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p><a class="reference internal" href="_images/bayes_cartoon.png"><img alt="_images/bayes_cartoon.png" src="_images/bayes_cartoon.png" style="width: 600px;" /></a></p></td>
</tr>
</tbody>
</table>
<p>Most of physicists we are bayesians, but we think of ourselves as frequentists.</p>
<p>Consider the discovery of the Higgs.</p>
<p>A frequentist claims: <em>LHC data strongly disagrees with the hypothesis of the SM without the Higgs and agrees with a Higgs.</em></p>
<p>While a Bayesian claims: <em>We have discovered the Higgs!</em></p>
<p>A joke in HEP (L. Lyons quotation?):</p>
<p>Frequentists use implecable logic to answer questions that nobody cares about.</p>
<p>Bayesians address the questions that everyone is interested on using assumptions that nobody believes.</p>
<div class="section" id="likelihood-and-posterior-probability-an-example">
<h3>Likelihood and posterior  probability - an example<a class="headerlink" href="#likelihood-and-posterior-probability-an-example" title="Permalink to this heading">#</a></h3>
<p>Let’s consider the basic measurement case seen by a frequentist and by a bayesian.</p>
<p>We just measure a quantity <span class="math notranslate nohighlight">\(\mu\)</span> using a set of data <span class="math notranslate nohighlight">\(x\)</span>. Data <span class="math notranslate nohighlight">\(x\)</span> are gaussian distributed with known sigma, <span class="math notranslate nohighlight">\(\sigma\)</span>. We want to estimate <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>.</p>
<p>For example, <span class="math notranslate nohighlight">\(x\)</span> is a set of values from a gaussian distribution with mean zero and sigma one. How to estimate <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>?</p>
<p>Obvious, it is the average! And the uncertanty in the average is <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of events in our sample!</p>
<p><strong>Example</strong>: Generate a n-size sample with <span class="math notranslate nohighlight">\(x\)</span> values generated random from a normal gaussian. The likelihood function is drawn as a function of <span class="math notranslate nohighlight">\(\mu\)</span>. The best-estimate <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is the value where the likelihood is maximum.</p>
<div class="math notranslate nohighlight">
\[
- 2 \log \mathcal{L}({\bf x} | \mu ) = - 2 \sum_{i=1}^n \log g(x_i | \mu)
\]</div>
<p><strong>Explore:</strong> Generate <span class="math notranslate nohighlight">\(n\)</span> samples of a normal. Compute the likelihood as function of the mean (fix sigma of the gaussian to 1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nsize</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsize</span><span class="p">)</span>
<span class="n">htexam</span><span class="o">.</span><span class="n">normal_likelihood</span><span class="p">(</span><span class="n">xs</span><span class="p">);</span>
<span class="c1">#plt.yscale(&#39;log&#39;);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mu mean : -0.12243391198311748 , mu std : 0.31622776601683794
</pre></div>
</div>
<img alt="_images/ta_basic_concepts_39_1.png" src="_images/ta_basic_concepts_39_1.png" />
</div>
</div>
</div>
<div class="section" id="posterior-probability-in-bayesian-statistics">
<h3>Posterior probability in bayesian statistics<a class="headerlink" href="#posterior-probability-in-bayesian-statistics" title="Permalink to this heading">#</a></h3>
<p>Bayesians improve a prior probability, “knowledge”, of an hypothesis using data.</p>
<p>Suppose that we have an ensemble of possible hypotheses <span class="math notranslate nohighlight">\(\mu\)</span>, each one with a prior probability <span class="math notranslate nohighlight">\(\pi(\mu)\)</span>.</p>
<p>Of course:</p>
<div class="math notranslate nohighlight">
\[
\int \pi(\mu) \, \mathrm{d}\mu = 1
\]</div>
<p>Or, if the hypotheses are discrete:</p>
<div class="math notranslate nohighlight">
\[
\sum \pi(\mu) = 1
\]</div>
<p>Given some data, <span class="math notranslate nohighlight">\(x\)</span>, Bayes’ theorem allow us to compute the <strong>posterior probability</strong>.</p>
<p>The posterior probability,  <span class="math notranslate nohighlight">\(p(\mu|x)\)</span>, of <span class="math notranslate nohighlight">\(\mu\)</span> given <span class="math notranslate nohighlight">\(x\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
p(\mu|x) = \frac{p(x|\mu) \, \pi(\mu)}{ p(x)}
\]</div>
<p>That is:</p>
<div class="math notranslate nohighlight">
\[
p(\mu|x) = \frac{p(x|\mu) \, \pi(\mu)}{\int p(x|\mu) \pi(\mu) \, \mathrm{d}\mu}
\]</div>
<p>if the hypotheses are discrete:</p>
<div class="math notranslate nohighlight">
\[
p(\mu|x) = \frac{p(x|\mu) \, \pi(\mu)}{\sum p(x|\mu) \pi(\mu)}
\]</div>
<p><strong>Example:</strong> Consider the example above. An sample of <span class="math notranslate nohighlight">\(n\)</span> measurements from a normal distribution. Obtain the posterior probability of the mean, if the sigma is know.</p>
<p>Consider a unique data <span class="math notranslate nohighlight">\(x_0\)</span>, from a gaussian distribution <span class="math notranslate nohighlight">\(g(x | \mu, \sigma = 1)\)</span>, and a ‘reasonable’ uniform prior <span class="math notranslate nohighlight">\(\pi(\mu) = 1\)</span>.</p>
<p>The posterior probability using bayes is:</p>
<div class="math notranslate nohighlight">
\[
p(\mu | x_0) = \frac{g( x_0 | \mu, \sigma = 1) \, \pi(\mu)}{ \int g(x_0 | \mu, \sigma = 1)  \, \pi(\mu) \, \mathrm{d} \mu } = g(x_0 | \mu, \sigma = 1)
\]</div>
<p>that is, a gaussian with sigma unity and centered at <span class="math notranslate nohighlight">\(x_0\)</span>!</p>
<p><strong>Exercise:</strong> Show that the posterior of n measurements, <span class="math notranslate nohighlight">\(x\)</span>, normal distributed, <span class="math notranslate nohighlight">\(g(x | \mu, \sigma)\)</span>, with a flat prior on <span class="math notranslate nohighlight">\(\mu\)</span>, and known <span class="math notranslate nohighlight">\(\sigma\)</span>, is a gaussian with mean <span class="math notranslate nohighlight">\(\mu = \bar{x}\)</span>, the average of the <span class="math notranslate nohighlight">\(n\)</span> <span class="math notranslate nohighlight">\(x\)</span> measurements, and the sigma <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nsize</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsize</span><span class="p">)</span>
<span class="c1">#print(xs)</span>
<span class="n">htexam</span><span class="o">.</span><span class="n">normal_posterior</span><span class="p">(</span><span class="n">xs</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>posterior integral 1.007
</pre></div>
</div>
<img alt="_images/ta_basic_concepts_46_1.png" src="_images/ta_basic_concepts_46_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nsize</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">xs</span>    <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsize</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="n">htexam</span><span class="o">.</span><span class="n">normal_likelihood</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span> <span class="n">htexam</span><span class="o">.</span><span class="n">normal_posterior</span><span class="p">(</span><span class="n">xs</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mu mean : 0.4027521257624195 , mu std : 0.31622776601683794
posterior integral 1.000
</pre></div>
</div>
<img alt="_images/ta_basic_concepts_47_1.png" src="_images/ta_basic_concepts_47_1.png" />
</div>
</div>
<p><strong>Exercise:</strong> Show that the posterior of a normal prior, <span class="math notranslate nohighlight">\(\pi(\mu | \mu_0, \sigma_{\mu_0})\)</span>,
and a normal likelihood, <span class="math notranslate nohighlight">\(p(x |\mu, \sigma)\)</span>, for n-measurements, <span class="math notranslate nohighlight">\(x\)</span>, is a normal distribution with mean, <span class="math notranslate nohighlight">\(\mu'\)</span>, and sigma, <span class="math notranslate nohighlight">\(\sigma_{\mu'}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{\sigma^2_{\mu'}} = \frac{n}{\sigma^2} + \frac{1}{\sigma^2_{\mu_0}}, 
\;\;\;
\mu' = \sigma^2_{\mu'} \left( \frac{\mu_0}{\sigma^2_{\mu_0}} + \frac{\sum_i x_i}{\sigma^2} \right)
\]</div>
<p><strong>Exercise:</strong> There are several dices in a box, with 4, 6, 12 and 24 sides. We randomly pick one dice and we roll it four times. The outcomes are <span class="math notranslate nohighlight">\(\{1,4,5,2\}\)</span>. What is the posterior probability that the selected dice has 4, 6, 12 or 24 sides? What is the posterior probability if we roll it twice again and wet get now 6 and 1?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">measurements</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">hpriors</span>      <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]</span>
<span class="k">for</span> <span class="n">meas</span> <span class="ow">in</span> <span class="n">measurements</span><span class="p">:</span>
    <span class="n">hposteriors</span> <span class="o">=</span> <span class="n">htexam</span><span class="o">.</span><span class="n">dice_posterior</span><span class="p">(</span><span class="n">meas</span><span class="p">,</span> <span class="n">hpriors</span><span class="p">)</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;data :&#39;</span><span class="p">,</span> <span class="n">meas</span><span class="p">,</span> <span class="s1">&#39; posteriors: &#39;</span><span class="p">,</span> <span class="n">hposteriors</span><span class="p">)</span>
    <span class="n">hpriors</span> <span class="o">=</span> <span class="n">hposteriors</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
data : 1  posteriors:  [0.46153846 0.30769231 0.15384615 0.07692308]
!
data : 4  posteriors:  [0.63157895 0.28070175 0.07017544 0.01754386]
!
data : 5  posteriors:  [0.         0.87671233 0.10958904 0.01369863]
!
data : 2  posteriors:  [0.         0.93772894 0.05860806 0.003663  ]
!
data : 6  posteriors:  [0.00000000e+00 9.68779565e-01 3.02743614e-02 9.46073794e-04]
!
data : 1  posteriors:  [0.00000000e+00 9.84378755e-01 1.53809180e-02 2.40326845e-04]
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Neutrinos - USC</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="ta_hypothesis_test.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Hypothesis testing, simple case</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By José Ángel Hernando Morata<br/>
  
      &copy; Copyright USC - J.A. Hernando - 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>